{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from trainer import Trainer\n",
    "from dataset import SpectrogramReader, Dataset, DataLoader, logger\n",
    "from dcnet import DCNet, DCNetDecoder\n",
    "from utils import nfft, parse_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!export WANDB_NOTEBOOK_NAME=experiments\n",
    "# !pip install wandb --upgrade\n",
    "# !pip install -U pip\n",
    "!echo $WANDB_NOTEBOOK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: tyteotv4\n",
      "Sweep URL: https://app.wandb.ai/kantologist/SpeakerSeparation/sweeps/tyteotv4\n"
     ]
    }
   ],
   "source": [
    "# # Weight and Bias configuration\n",
    "\n",
    "# run = wandb.init(project=\"Speaker Separation\")\n",
    "# wandb_config = run.config\n",
    "# wandb_config.num_epoches = 1\n",
    "# wandb_config.lr = 0.001\n",
    "\n",
    "# Configure the sweep â€“ specify the parameters to search through, the search strategy, the optimization metric et all.\n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'values': [1e-2, 1e-3]\n",
    "        },\n",
    "        'optim': {\n",
    "            'values': ['adam', 'nadam', 'sgd', 'rmsprop']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, entity=\"kantologist\", project=\"SpeakerSeparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uttloader(scp_config, reader_kwargs, loader_kwargs, train=True):\n",
    "    mix_reader = SpectrogramReader(scp_config['mixture'], **reader_kwargs)\n",
    "    target_reader = [\n",
    "        SpectrogramReader(scp_config[spk_key], **reader_kwargs)\n",
    "        for spk_key in scp_config if spk_key[:3] == 'spk'\n",
    "    ]\n",
    "    dataset = Dataset(mix_reader, target_reader)\n",
    "#     print(dataset[12][0].shape)\n",
    "#     print(dataset[12][1][0].shape)\n",
    "#     print(dataset[12][1][1].shape)\n",
    "    # modify shuffle status\n",
    "    loader_kwargs[\"shuffle\"] = train\n",
    "    # validate perutt if needed\n",
    "    # if not train:\n",
    "    #     loader_kwargs[\"batch_size\"] = 1\n",
    "    # if validate, do not shuffle\n",
    "    utt_loader = DataLoader(dataset, **loader_kwargs)\n",
    "    return utt_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + '/weights/'\n",
    "# loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "def loss(x, x_, mu, var):\n",
    "#     likelihood = F.binary_cross_entropy(x_, x, reduction=\"sum\")\n",
    "    likelihood = F.mse_loss(x_, x, reduction=\"sum\")\n",
    "    kld = -0.5 * torch.sum(1 + var - mu.pow(2) - var.exp())\n",
    "    return likelihood + kld\n",
    "\n",
    "def optimizer(optim_config, lr_config, encoder, decoder):\n",
    "    if optim_config == 'adam':\n",
    "        optimizer_ = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr_config)\n",
    "    if optim_config == 'nadam':\n",
    "        optimizer_ = torch.optim.Nadam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr_config)\n",
    "    if optim_config == 'sgd':\n",
    "        optimizer_ = torch.optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=lr_config)\n",
    "    if optim_config == 'rmsprop':\n",
    "        optimizer_ = torch.optim.RMSprop(list(encoder.parameters()) + list(decoder.parameters()), lr=lr_config)\n",
    "    return optimizer_\n",
    "\n",
    "def train():\n",
    "    class Args:\n",
    "        config = \"conf/train.yaml\"\n",
    "        debug = True\n",
    "    args = Args\n",
    "    \n",
    "    # Weight and Bias configuration\n",
    "    \n",
    "    config_defaults = {\n",
    "        'num_epoches': 1,\n",
    "        'lr': 1e-3,\n",
    "        'optim': 'adam'\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults)\n",
    "    wandb_config = wandb.config\n",
    "#     wandb_config.num_epoches = 1\n",
    "#     wandb_config.lr = 0.001\n",
    "#     wandb_config.optim = 'adam' \n",
    "    \n",
    "\n",
    "#     debug = args.debug\n",
    "    debug = wandb_config.num_epoches\n",
    "    logger.info(\n",
    "        \"Start training in {} model\".format('debug' if debug else 'normal'))\n",
    "    num_bins, config_dict = parse_yaml(args.config)\n",
    "    reader_conf = config_dict[\"spectrogram_reader\"]\n",
    "    loader_conf = config_dict[\"dataloader\"]\n",
    "    dcnnet_conf = config_dict[\"dcnet\"]\n",
    "\n",
    "    batch_size = loader_conf[\"batch_size\"]\n",
    "    logger.info(\n",
    "        \"Training in {}\".format(\"per utterance\" if batch_size == 1 else\n",
    "                                '{} utterance per batch'.format(batch_size)))\n",
    "\n",
    "    train_loader = uttloader(\n",
    "        config_dict[\"train_scp_conf\"]\n",
    "        if not debug else config_dict[\"debug_scp_conf\"],\n",
    "        reader_conf,\n",
    "        loader_conf,\n",
    "        train=True)\n",
    "#    valid_loader = uttloader(\n",
    "#        config_dict[\"valid_scp_conf\"]\n",
    "#        if not debug else config_dict[\"debug_scp_conf\"],\n",
    "#        reader_conf,\n",
    "#        loader_conf,\n",
    "#        train=False)\n",
    "#    checkpoint = config_dict[\"trainer\"][\"checkpoint\"]\n",
    "#    logger.info(\"Training for {} epoches -> {}...\".format(\n",
    "#        args.num_epoches, \"default checkpoint\"\n",
    "#        if checkpoint is None else checkpoint))\n",
    "#     loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    dcnet = DCNet(num_bins, **dcnnet_conf)\n",
    "    dcnet_decode = DCNetDecoder(num_bins, **dcnnet_conf)\n",
    "    optimizer_ = optimizer(wandb_config.optim, wandb_config.lr, dcnet, dcnet_decode)\n",
    "\n",
    "    for epoch in range(wandb_config.num_epoches):\n",
    "        for i in range(3):\n",
    "            for j, a in enumerate(iter(train_loader)):\n",
    "                input_ = a[i]\n",
    "#                 print(\"Input\", input_.shape)\n",
    "#                 z, mu, var = dcnet(input_)\n",
    "#                 print(z.shape)\n",
    "#                 print(mu.shape)\n",
    "#                 print(var.shape)\n",
    "#                 decode_out = dcnet_decode(z)\n",
    "#                 print(\"Out\", decode_out.shape)\n",
    "#                 loss_ = loss(input_, decode_out, mu, var)\n",
    "#                 print(\"Loss\", loss_.item())\n",
    "                \n",
    "                if i != 0:\n",
    "                    input_ = torch.mul(a[i].float(),a[0])\n",
    "                else:\n",
    "                    input_ = a[i]\n",
    "    #             print(\"Input\", input_)\n",
    "                z, mu, var = dcnet(input_)\n",
    "                decode_out = dcnet_decode(z)\n",
    "                optimizer_.zero_grad()\n",
    "                loss_ = loss(input_, decode_out, mu, var)\n",
    "#                 wandb.log({'epoch': epoch, 'Speaker '+ str(i) + ' loss': loss_})\n",
    "                wandb.log({'Speaker '+ str(i) + ' loss': loss_})\n",
    "#                 print(\"Loss\", loss_.item())\n",
    "                loss_.backward()\n",
    "                optimizer_.step()\n",
    "\n",
    "#                 decode_out = torch.sigmoid(decode_out)\n",
    "#                 print(\"decoder output\", decode_out)\n",
    "                if j == 50:\n",
    "                    break\n",
    "    torch.save(dcnet.state_dict(), PATH + \"encoder_\" + str(i))\n",
    "    torch.save(dcnet_decode.state_dict(), PATH + \"decoder_\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/kantologist/SpeakerSeparation\" target=\"_blank\">https://app.wandb.ai/kantologist/SpeakerSeparation</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/kantologist/SpeakerSeparation/runs/d4ffwexi\" target=\"_blank\">https://app.wandb.ai/kantologist/SpeakerSeparation/runs/d4ffwexi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "2020-02-12 14:23:54,572 [<ipython-input-19-50248903de17>:45 - INFO ] Start training in debug model\n",
      "2020-02-12 14:23:54,581 [<ipython-input-19-50248903de17>:54 - INFO ] Training in per utterance\n",
      "2020-02-12 14:23:54,586 [/home/ubuntu/deep-clustering/dataset.py:40 - INFO ] Create SpectrogramReader for ./data/2spk/test/wav8k_min_mix.scp with 3000 utterances\n",
      "2020-02-12 14:23:54,591 [/home/ubuntu/deep-clustering/dataset.py:40 - INFO ] Create SpectrogramReader for ./data/2spk/test/wav8k_min_s1.scp with 3000 utterances\n",
      "2020-02-12 14:23:54,596 [/home/ubuntu/deep-clustering/dataset.py:40 - INFO ] Create SpectrogramReader for ./data/2spk/test/wav8k_min_s2.scp with 3000 utterances\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    config = \"conf/train.yaml\"\n",
    "    debug = True\n",
    "#     num_epoches = wandb_config.num_epoches\n",
    "# args ={\"config\":\"conf/train.yaml\", \"debug\":False, \"num_epoches\":20} \n",
    "# print(args[\"debug\"])\n",
    "# wandb.agent(sweep_id, train)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.zeros((24,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n"
     ]
    }
   ],
   "source": [
    "a = a.unsqueeze(dim=0)\n",
    "print(a.shape)\n",
    "a = a.view(-1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/deep-clustering/weights'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() + '/weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
